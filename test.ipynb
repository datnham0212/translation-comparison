{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "from pyvi import ViTokenizer\n",
    "from difflib import SequenceMatcher\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Initialize the translator\n",
    "translator = Translator()\n",
    "\n",
    "# Load your English captions CSV file\n",
    "english_captions = pd.read_csv('forrest_gump_transcript_en.csv')\n",
    "human_translations = pd.read_csv('forrest_gump_transcript_vn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Translate English captions to Vietnamese\n",
    "english_captions['translated_vietnamese'] = english_captions['Transcript Line'].head(3).apply(\n",
    "    lambda x: translator.translate(x, src='en', dest='vi').text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Tokenize the machine-translated Vietnamese text\n",
    "english_captions['tokenized_vietnamese'] = english_captions['translated_vietnamese'].head(3).apply(ViTokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Tokenize the human-translated Vietnamese captions\n",
    "human_translations['tokenized_caption'] = human_translations['Transcript Line'].head(3).apply(ViTokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define a function to calculate similarity ratio (Levenshtein-based)\n",
    "def similarity(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define a function to calculate BLEU score\n",
    "def bleu_score(reference, candidate):\n",
    "    return sentence_bleu([reference.split()], candidate.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Python312\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Apply similarity and BLEU score comparisons between machine and human translations\n",
    "english_captions['similarity'] = english_captions.head(3).apply(\n",
    "    lambda row: similarity(row['tokenized_vietnamese'], human_translations.loc[row.name, 'tokenized_caption']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "english_captions['bleu_score'] = english_captions.head(3).apply(\n",
    "    lambda row: bleu_score(human_translations.loc[row.name, 'tokenized_caption'], row['tokenized_vietnamese']),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Save the comparison results to a CSV file\n",
    "english_captions.head(3).to_csv('pyvi_comparison_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Line Number                                   Transcript Line  \\\n",
      "0            1         Hello. My name's Forrest... Forrest Gump.   \n",
      "1            2                          Do you want a chocolate?   \n",
      "2            3  I could eat about a million and a half of these.   \n",
      "\n",
      "                               translated_vietnamese  \\\n",
      "0      Xin chào.Tên tôi là Forrest ... Forrest Gump.   \n",
      "1                          Bạn có muốn một sô cô la?   \n",
      "2  Tôi có thể ăn khoảng một triệu rưỡi trong số này.   \n",
      "\n",
      "                                tokenized_vietnamese  similarity  \\\n",
      "0   Xin chào . Tên tôi là Forrest ... Forrest_Gump .    0.895833   \n",
      "1                         Bạn có muốn một sô cô la ?    0.562500   \n",
      "2  Tôi có_thể ăn khoảng một triệu rưỡi trong số n...    0.672897   \n",
      "\n",
      "      bleu_score  \n",
      "0   7.598357e-01  \n",
      "1  7.176382e-155  \n",
      "2  7.428368e-155  \n"
     ]
    }
   ],
   "source": [
    "# Print some of the results for quick inspection\n",
    "print(english_captions[['Line Number', 'Transcript Line', 'translated_vietnamese', 'tokenized_vietnamese', 'similarity', 'bleu_score']].head(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
